{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以utf-8编码格式加载csv文件:/HL_user01/processed_emr_datas/yanke/new_最终处理并合并后数据.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3933/3933 [00:00<00:00, 9397.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以utf-8编码格式加载csv文件:/HL_user01/processed_emr_datas/zhongyike/new_最终处理并合并后数据.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4994/4994 [00:00<00:00, 7731.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以utf-8编码格式加载csv文件:/HL_user01/processed_emr_datas/neifenmi/new_最终处理并合并后数据.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4945/4945 [00:00<00:00, 4988.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以utf-8编码格式加载csv文件:/HL_user01/processed_emr_datas/xiaohuaneike/new_最终处理并合并后数据.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4983/4983 [00:00<00:00, 6399.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以utf-8编码格式加载csv文件:/HL_user01/processed_emr_datas/huxineike/new_最终处理并合并后数据.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4972/4972 [00:01<00:00, 3856.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以utf-8编码格式加载csv文件:/HL_user01/processed_emr_datas/shenjingneike/new_最终处理并合并后数据.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4602/4602 [00:00<00:00, 5278.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以utf-8编码格式加载csv文件:/HL_user01/processed_emr_datas/shenzangneike/new_最终处理并合并后数据.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4983/4983 [00:01<00:00, 4605.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以utf-8编码格式加载csv文件:/HL_user01/processed_emr_datas/ruxianwaike/new_最终处理并合并后数据.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4978/4978 [00:00<00:00, 6967.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以utf-8编码格式加载csv文件:/HL_user01/processed_emr_datas/fuke/new_最终处理并合并后数据.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4982/4982 [00:00<00:00, 6790.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以utf-8编码格式加载csv文件:/HL_user01/processed_emr_datas/jiazhuangxianxueguanwaike/new_最终处理并合并后数据.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4963/4963 [00:00<00:00, 8310.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以utf-8编码格式加载csv文件:/HL_user01/processed_emr_datas/zhongliuke/new_最终处理并合并后数据.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4995/4995 [00:00<00:00, 8910.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以utf-8编码格式加载csv文件:/HL_user01/processed_emr_datas/xiaoerke/new_最终处理并合并后数据.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3891/3891 [00:00<00:00, 6741.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以utf-8编码格式加载csv文件:/HL_user01/processed_emr_datas/erbihouke/new_最终处理并合并后数据.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3711/3711 [00:00<00:00, 7940.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以utf-8编码格式加载csv文件:/HL_user01/processed_emr_datas/shenjingwaike/new_最终处理并合并后数据.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4959/4959 [00:00<00:00, 7588.50it/s]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "from commons.utils import *\n",
    "from commons.preprocess import *\n",
    "from commons.constants import *\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from bs4 import BeautifulSoup,Tag\n",
    "import bs4\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import json\n",
    "import jsonlines\n",
    "from datetime import timedelta,datetime\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "import py_cyxj_202407_gys as cyxj\n",
    "\n",
    "\n",
    "data_name = 'new_最终处理并合并后数据.csv'\n",
    "data_dir='/HL_user01/processed_emr_datas/'\n",
    "zylshs = os.listdir(data_dir)\n",
    "json_content=[]\n",
    "for zylsh  in zylshs:\n",
    "    if zylsh !='read.ipynb'and zylsh !='病历模板_processed.json'and zylsh !='得到模板.ipynb'and zylsh !='病历模板.json' and zylsh!='weichangwaike':\n",
    "        csv_data_path = os.path.join(data_dir,zylsh,data_name)\n",
    "        datas = cyxj.load_excel_csv(csv_data_path)    \n",
    "        datas.fillna('',inplace=True)\n",
    "\n",
    "        for col in datas.columns[1:]:\n",
    "                datas[col] = datas[col].apply(transfer_value)\n",
    "\n",
    "\n",
    "        jianyan_name = []\n",
    "        for i in tqdm(range(datas.shape[0])):\n",
    "                jianyan_list = datas.iloc[i,:].copy().iat[11]\n",
    "                \n",
    "                for report in jianyan_list:\n",
    "                    for test in report['检验详情']:\n",
    "                        test_name = test['检验指标'].strip()\n",
    "                        jianyan_name.append(test_name)\n",
    "        \n",
    "        jianyan_dict={\n",
    "            '科室':zylsh,\n",
    "            '检验统计':Counter(jianyan_name),\n",
    "        }\n",
    "        json_content.append(jianyan_dict)\n",
    "        \n",
    "        with open(\"/HL_user01/psh/检验统计/检验统计_各科室总.json\", \"w\", encoding='utf-8') as f:\n",
    "            # json.dump(dict_, f)  # 写为一行\n",
    "            json.dump(json_content, f, indent=2, ensure_ascii=False)  # 写为多行\n",
    "\n",
    "                # print('##'+zylsh+'##')\n",
    "                # print(Counter(jianyan_name))\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以utf-8编码格式加载csv文件:/HL_user01/processed_emr_datas/yanke/new_最终处理并合并后数据.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3933/3933 [00:00<00:00, 8551.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以utf-8编码格式加载csv文件:/HL_user01/processed_emr_datas/zhongyike/new_最终处理并合并后数据.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4994/4994 [00:00<00:00, 6732.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以utf-8编码格式加载csv文件:/HL_user01/processed_emr_datas/neifenmi/new_最终处理并合并后数据.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4945/4945 [00:01<00:00, 3143.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以utf-8编码格式加载csv文件:/HL_user01/processed_emr_datas/xiaohuaneike/new_最终处理并合并后数据.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4983/4983 [00:01<00:00, 3295.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以utf-8编码格式加载csv文件:/HL_user01/processed_emr_datas/huxineike/new_最终处理并合并后数据.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4972/4972 [00:01<00:00, 3863.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以utf-8编码格式加载csv文件:/HL_user01/processed_emr_datas/shenjingneike/new_最终处理并合并后数据.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4602/4602 [00:01<00:00, 3418.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以utf-8编码格式加载csv文件:/HL_user01/processed_emr_datas/shenzangneike/new_最终处理并合并后数据.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4983/4983 [00:01<00:00, 3952.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以utf-8编码格式加载csv文件:/HL_user01/processed_emr_datas/ruxianwaike/new_最终处理并合并后数据.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4978/4978 [00:01<00:00, 2651.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以utf-8编码格式加载csv文件:/HL_user01/processed_emr_datas/fuke/new_最终处理并合并后数据.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4982/4982 [00:00<00:00, 5871.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以utf-8编码格式加载csv文件:/HL_user01/processed_emr_datas/jiazhuangxianxueguanwaike/new_最终处理并合并后数据.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4963/4963 [00:01<00:00, 4000.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以utf-8编码格式加载csv文件:/HL_user01/processed_emr_datas/zhongliuke/new_最终处理并合并后数据.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4995/4995 [00:00<00:00, 7938.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以utf-8编码格式加载csv文件:/HL_user01/processed_emr_datas/xiaoerke/new_最终处理并合并后数据.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3891/3891 [00:00<00:00, 5922.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以utf-8编码格式加载csv文件:/HL_user01/processed_emr_datas/erbihouke/new_最终处理并合并后数据.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3711/3711 [00:00<00:00, 6925.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "以utf-8编码格式加载csv文件:/HL_user01/processed_emr_datas/shenjingwaike/new_最终处理并合并后数据.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4959/4959 [00:01<00:00, 2677.78it/s]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "from commons.utils import *\n",
    "from commons.preprocess import *\n",
    "from commons.constants import *\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from bs4 import BeautifulSoup,Tag\n",
    "import bs4\n",
    "from copy import deepcopy\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "import json\n",
    "import jsonlines\n",
    "from datetime import timedelta,datetime\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from copy import deepcopy\n",
    "from collections import Counter\n",
    "import py_cyxj_202407_gys as cyxj\n",
    "\n",
    "\n",
    "data_name = 'new_最终处理并合并后数据.csv'\n",
    "data_dir='/HL_user01/processed_emr_datas/'\n",
    "zylshs = os.listdir(data_dir)\n",
    "#output_path='/HL_user01/psh/检验统计/'\n",
    "\n",
    "for zylsh  in zylshs:\n",
    "    if zylsh !='read.ipynb'and zylsh !='病历模板_processed.json'and zylsh !='得到模板.ipynb'and zylsh !='病历模板.json'and zylsh!='weichangwaike':\n",
    "        json_content = []\n",
    "        csv_data_path = os.path.join(data_dir,zylsh,data_name)\n",
    "        datas = cyxj.load_excel_csv(csv_data_path)    \n",
    "        datas.fillna('',inplace=True)\n",
    "        output_name='/HL_user01/psh/检验统计/'+zylsh+'检验统计_分病人.json'\n",
    "\n",
    "        for col in datas.columns[1:]:\n",
    "                datas[col] = datas[col].apply(transfer_value)\n",
    "\n",
    "\n",
    "        \n",
    "        for i in tqdm(range(datas.shape[0])):\n",
    "                jianyan_list = datas.iloc[i,:].copy().iat[11]\n",
    "                jianyan_name = []\n",
    "                \n",
    "                for report in jianyan_list:\n",
    "                    _id=report['检验详情'][0]['住院流水号']\n",
    "                    for test in report['检验详情']:\n",
    "                        test_name = test['检验指标'].strip()\n",
    "                        jianyan_name.append(test_name)\n",
    "\n",
    "                jianyan_dict={\n",
    "                    '病人流水号':_id,\n",
    "                    '检验统计':Counter(jianyan_name),\n",
    "                }  \n",
    "                \n",
    "                json_content.append(jianyan_dict)\n",
    "\n",
    "        with open(output_name, \"w\", encoding='utf-8') as f:\n",
    "            json.dump(json_content, f,indent=2, ensure_ascii=False)  # 写为一行\n",
    "            #json.dump(jianyan_dict, f, indent=2, ensure_ascii=False)  # 写为多行\n",
    "                                \n",
    "               \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_factory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
